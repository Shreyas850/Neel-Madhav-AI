from llama_cpp import Llama
import os
import sys

# 1. Check for Model
model_name = "Phi-3-mini-4k-instruct-q4.gguf"
if not os.path.exists(model_name):
    print(f"‚ùå ERROR: {model_name} not found.")
    print("üëâ Run 'python model_down.py' first!")
    sys.exit()

print("üß™ TESTING GPU CONNECTION...")

try:
    # 2. Try to load model on GPU
    llm = Llama(
        model_path=model_name,
        n_gpu_layers=-1,      # <--- The Magic Switch (All layers to GPU)
        verbose=True          # <--- Show us the internal logs
    )
    
    print("\n" + "="*40)
    print("‚úÖ SUCCESS! The Brain loaded successfully.")
    print("="*40)
    print("üìù CHECK THE LOGS ABOVE FOR:")
    print("   ‚Ä¢ 'BLAS = 1' (Means GPU Acceleration is ON)")
    print("   ‚Ä¢ 'ggml_cuda_init: found 1 CUDA devices' (Means GTX 1650 found)")
    print("="*40)

except Exception as e:
    print("\n‚ùå CRITICAL ERROR:")
    print(e)
    print("\nüí° TIP: Did you install the 'cu124' wheel correctly?")